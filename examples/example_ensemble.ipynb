{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, \"../\")\n",
    "from rurage import RAGEModelConfig, RAGESetConfig, RAGEvaluator, RAGEnsemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each model that needs to be evaluated, you need to initialize a config containing:\n",
    "# * the name of the column with the context on which the answer was generated\n",
    "# * the name of the column with the generated model answer\n",
    "models_cfg = []\n",
    "models_cfg.append(\n",
    "    RAGEModelConfig(\n",
    "        context_col=\"context_top5\", answer_col=\"GPT4o_top5\"\n",
    "    )\n",
    ")\n",
    "models_cfg.append(\n",
    "    RAGEModelConfig(\n",
    "        context_col=\"context_top5\", answer_col=\"gpt35_top5\"\n",
    "    )\n",
    ")\n",
    "models_cfg.append(\n",
    "    RAGEModelConfig(\n",
    "        context_col=\"context_top5\", answer_col=\"cotype_light_top5\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_set = pd.read_csv(\"golden_set.csv\")\n",
    "\n",
    "# Dataset contains multiclass markup, but we want to use only 0/1 labels\n",
    "validation_set = validation_set[\n",
    "    (validation_set[\"r_GPT4o_top5\"] != \"0,5\")\n",
    "    & (validation_set[\"r_gpt35_top5\"] != \"0,5\")\n",
    "    & (validation_set[\"r_cotype_light_top5\"] != \"0,5\")\n",
    "].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the configuration of the evaluation set:\n",
    "# * validation set pd.Daraframe\n",
    "# * The name of the question column\n",
    "# * The name of the golden answer column\n",
    "# * The list of model configs\n",
    "validation_set_cfg = RAGESetConfig(\n",
    "    golden_set=validation_set,\n",
    "    question_col=\"Question\",\n",
    "    golden_answer_col=\"Golden Answer\",\n",
    "    models_cfg=models_cfg,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the evaluator\n",
    "rager = RAGEvaluator(golden_set_cfg=validation_set_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting correctness evaluation\n",
      "Initializing the NLI model: MoritzLaurer/mDeBERTa-v3-base-xnli-multilingual-nli-2mil7\n",
      "Initializing the similarity model: intfloat/multilingual-e5-large\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3e3f5a5e9e544ab8f332ac13900df42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Model #:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "total_report, pointwise_reports = rager.evaluate_correctness(pointwise_report=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\n",
    "    validation_set[\"r_GPT4o_top5\"].values,\n",
    "    validation_set[\"r_gpt35_top5\"].values,\n",
    "    validation_set[\"r_cotype_light_top5\"].values,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble = RAGEnsemble(ensemble_type=\"correctness\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = ensemble.prepare_data_for_study(\n",
    "    pointwise_reports=pointwise_reports, labels=labels, set_config=validation_set_cfg\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting threshold optimization\n",
      "Threshold for the correctness task: 0.35035035035035034\n"
     ]
    }
   ],
   "source": [
    "ensemble.fit(X_train, y_train, X_test, y_test, optimize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = ensemble.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.81      0.86       333\n",
      "           1       0.87      0.95      0.91       450\n",
      "\n",
      "    accuracy                           0.89       783\n",
      "   macro avg       0.90      0.88      0.89       783\n",
      "weighted avg       0.89      0.89      0.89       783\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test.astype(int), y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example below shows inference on the one sample, but it is possible to do batch inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>context_top5</th>\n",
       "      <th>Golden Answer</th>\n",
       "      <th>cotype_light_top5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>в каком году была основана компания гугл</td>\n",
       "      <td>Google LLC — транснациональная корпорация из С...</td>\n",
       "      <td>Компания Google была основана 4 сентября 1998 ...</td>\n",
       "      <td>Компания Google была основана в 1998 году.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   Question  \\\n",
       "0  в каком году была основана компания гугл   \n",
       "\n",
       "                                        context_top5  \\\n",
       "0  Google LLC — транснациональная корпорация из С...   \n",
       "\n",
       "                                       Golden Answer  \\\n",
       "0  Компания Google была основана 4 сентября 1998 ...   \n",
       "\n",
       "                            cotype_light_top5  \n",
       "0  Компания Google была основана в 1998 году.  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_set = pd.read_csv(\"one_sample.csv\")\n",
    "validation_set[[\"Question\", \"context_top5\", \"Golden Answer\", \"cotype_light_top5\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_cfg = []\n",
    "models_cfg.append(\n",
    "    RAGEModelConfig(\n",
    "        context_col=\"context_top5\", answer_col=\"cotype_light_top5\"\n",
    "    )\n",
    ")\n",
    "\n",
    "validation_set_cfg = RAGESetConfig(\n",
    "    golden_set=validation_set,\n",
    "    question_col=\"Question\",\n",
    "    golden_answer_col=\"Golden Answer\",\n",
    "    models_cfg=models_cfg,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting correctness evaluation\n",
      "Initializing the NLI model: MoritzLaurer/mDeBERTa-v3-base-xnli-multilingual-nli-2mil7\n",
      "Initializing the similarity model: intfloat/multilingual-e5-large\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4815af0a9b69431ebeb47704a8a058ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Model #:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rager = RAGEvaluator(golden_set_cfg=validation_set_cfg)\n",
    "_, pointwise_reports = rager.evaluate_correctness(pointwise_report=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble = RAGEnsemble(ensemble_type=\"correctness\", model_path=\"rurage_ensemble.cbm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = ensemble.prepare_data_for_inference(data=pointwise_reports[0], set_config=validation_set_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nli</th>\n",
       "      <th>sim</th>\n",
       "      <th>unigram_overlap_precision</th>\n",
       "      <th>unigram_overlap_recall</th>\n",
       "      <th>unigram_overlap_f1</th>\n",
       "      <th>bigram_overlap_precision</th>\n",
       "      <th>bigram_overlap_recall</th>\n",
       "      <th>bigram_overlap_f1</th>\n",
       "      <th>rouge_precision</th>\n",
       "      <th>rouge_recall</th>\n",
       "      <th>rouge_f1</th>\n",
       "      <th>bleu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.974703</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.67032</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   nli       sim  unigram_overlap_precision  unigram_overlap_recall  \\\n",
       "0    0  0.974703                   0.714286                0.714286   \n",
       "\n",
       "   unigram_overlap_f1  bigram_overlap_precision  bigram_overlap_recall  \\\n",
       "0            0.714286                       0.5               0.428571   \n",
       "\n",
       "   bigram_overlap_f1  rouge_precision  rouge_recall  rouge_f1     bleu  \n",
       "0           0.461538              1.0      0.714286  0.833333  0.67032  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['1'], dtype=object)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensemble.predict(X)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
