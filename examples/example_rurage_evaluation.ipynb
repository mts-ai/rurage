{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "\n",
    "# sys.path.insert(0, \"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from rurage import RAGEvaluator, RAGESetConfig, RAGEModelConfig, report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. For each model that needs to be evaluated, you need to initialize a config containing:\n",
    "* the name of the column with the context on which the answer was generated\n",
    "* the name of the column with the generated model answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_cfg = []\n",
    "models_cfg.append(\n",
    "    RAGEModelConfig(\n",
    "        context_col=\"context_top5\", answer_col=\"GPT4o_top5\"\n",
    "    )\n",
    ")\n",
    "models_cfg.append(\n",
    "    RAGEModelConfig(\n",
    "        context_col=\"context_top5\", answer_col=\"gpt35_top5\"\n",
    "    )\n",
    ")\n",
    "models_cfg.append(\n",
    "    RAGEModelConfig(\n",
    "        context_col=\"context_top5\", answer_col=\"cotype_light_top5\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Initialize the configuration of the evaluation set:\n",
    "* Validation set pd.Dataframe\n",
    "* Name of the question column\n",
    "* Name of the golden answer column\n",
    "* List of model configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_set = pd.read_csv(\"golden_set.csv\")\n",
    "validation_set_cfg = RAGESetConfig(\n",
    "    golden_set=validation_set,\n",
    "    question_col=\"Question\",\n",
    "    golden_answer_col=\"Golden Answer\",\n",
    "    models_cfg=models_cfg,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Initialize the evaluator by passing the collected configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rager = RAGEvaluator(golden_set_cfg=validation_set_cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Run a comprehensive evalution (Correctness, Faithfulness, Relevance) for each model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting correctness evaluation\n",
      "The NLI model has alredy been loaded.\n",
      "The similarity model has alredy been loaded.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dc1ddab6e5b489f85e93d0e2d6ecd77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Model #:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting faithfulness evaluation\n",
      "The NLI model has alredy been loaded.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7509cec492344dbeb4e2a6a48b099796",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Model #:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting relevance evaluation\n",
      "The similarity model has alredy been loaded.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a04d3759d504ddfaba5f3462b1704af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Model #:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "comprehensive_report = rager.comprehensive_evaluation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.a. You can print a specific report using the special method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GPT4o_top5]\n",
      "\tEntailment score: 0.3719107896323086\n",
      "\tNeutral score: 0.28933092224231466\n",
      "\tContradiction score: 0.3387582881253767\n",
      "\n",
      "\n",
      "\tSimilarity score: 0.8344792\n",
      "\n",
      "\n",
      "\tToken overlap (1-gram) precision: 0.14285714285714285\n",
      "\tToken overlap (1-gram) recall: 0.11764705882352941\n",
      "\tToken overlap (1-gram) F1: 0.13333333333333333\n",
      "\n",
      "\n",
      "\tToken overlap (2-gram) precision: 0.0\n",
      "\tToken overlap (2-gram) recall: 0.0\n",
      "\tToken overlap (12-gram) F1: 0.0\n",
      "\n",
      "\n",
      "\tROUGE-L precision: 0.2857142857142857\n",
      "\tROUGE-L recall: 0.13333333333333333\n",
      "\tROUGE-L F1: 0.1904761904761905\n",
      "\n",
      "\n",
      "\tBLEU: 0.013349307984130428\n",
      "\n",
      "\n",
      "\n",
      "[gpt35_top5]\n",
      "\tEntailment score: 0.47860156720916214\n",
      "\tNeutral score: 0.26702833031946954\n",
      "\tContradiction score: 0.2543701024713683\n",
      "\n",
      "\n",
      "\tSimilarity score: 0.90089774\n",
      "\n",
      "\n",
      "\tToken overlap (1-gram) precision: 0.3333333333333333\n",
      "\tToken overlap (1-gram) recall: 0.23809523809523808\n",
      "\tToken overlap (1-gram) F1: 0.2857142857142857\n",
      "\n",
      "\n",
      "\tToken overlap (2-gram) precision: 0.1\n",
      "\tToken overlap (2-gram) recall: 0.058823529411764705\n",
      "\tToken overlap (12-gram) F1: 0.07692307692307691\n",
      "\n",
      "\n",
      "\tROUGE-L precision: 0.5\n",
      "\tROUGE-L recall: 0.3\n",
      "\tROUGE-L F1: 0.33333333333333337\n",
      "\n",
      "\n",
      "\tBLEU: 0.22313016014842982\n",
      "\n",
      "\n",
      "\n",
      "[cotype_light_top5]\n",
      "\tEntailment score: 0.4695599758890898\n",
      "\tNeutral score: 0.2820976491862568\n",
      "\tContradiction score: 0.24834237492465341\n",
      "\n",
      "\n",
      "\tSimilarity score: 0.9350007\n",
      "\n",
      "\n",
      "\tToken overlap (1-gram) precision: 0.5\n",
      "\tToken overlap (1-gram) recall: 0.3333333333333333\n",
      "\tToken overlap (1-gram) F1: 0.3846153846153846\n",
      "\n",
      "\n",
      "\tToken overlap (2-gram) precision: 0.18181818181818182\n",
      "\tToken overlap (2-gram) recall: 0.1\n",
      "\tToken overlap (12-gram) F1: 0.12903225806451615\n",
      "\n",
      "\n",
      "\tROUGE-L precision: 0.5\n",
      "\tROUGE-L recall: 0.4166666666666667\n",
      "\tROUGE-L F1: 0.4137931034482759\n",
      "\n",
      "\n",
      "\tBLEU: 0.3422780793550613\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for single_report in comprehensive_report[\"correctness_report\"]:\n",
    "    report.correctness_report(single_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.b. Also you can convert it to a dict:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import asdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RAGEReport(report_name='GPT4o_top5', entailment_score=0.3719107896323086, neutral_score=0.28933092224231466, contradiction_score=0.3387582881253767, similarity_score=0.8344792, unigram_overlap_precision=0.14285714285714285, unigram_overlap_recall=0.11764705882352941, unigram_overlap_f1=0.13333333333333333, bigram_overlap_precision=0.0, bigram_overlap_recall=0.0, bigram_overlap_f1=0.0, rouge_precision=0.2857142857142857, rouge_recall=0.13333333333333333, rouge_f1=0.1904761904761905, bleu_score=0.013349307984130428)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comprehensive_report[\"correctness_report\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'report_name': 'GPT4o_top5',\n",
       " 'entailment_score': 0.3719107896323086,\n",
       " 'neutral_score': 0.28933092224231466,\n",
       " 'contradiction_score': 0.3387582881253767,\n",
       " 'similarity_score': 0.8344792,\n",
       " 'unigram_overlap_precision': 0.14285714285714285,\n",
       " 'unigram_overlap_recall': 0.11764705882352941,\n",
       " 'unigram_overlap_f1': 0.13333333333333333,\n",
       " 'bigram_overlap_precision': 0.0,\n",
       " 'bigram_overlap_recall': 0.0,\n",
       " 'bigram_overlap_f1': 0.0,\n",
       " 'rouge_precision': 0.2857142857142857,\n",
       " 'rouge_recall': 0.13333333333333333,\n",
       " 'rouge_f1': 0.1904761904761905,\n",
       " 'bleu_score': 0.013349307984130428}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asdict(comprehensive_report[\"correctness_report\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. If there is no need to run a comprehensive evaluation, you can use different evaluations separately:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correctness_report = rager.evaluate_correctness()\n",
    "faithfulness_report = rager.evaluate_faithfulness()\n",
    "relevance_report = rager.evaluate_relevance()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. For the NLI and Similarity scores you can specify your own models. By default:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "correctness_report = rager.evaluate_correctness(\n",
    "    nli_model_name=\"MoritzLaurer/mDeBERTa-v3-base-xnli-multilingual-nli-2mil7\",\n",
    "    sim_model_name=\"intfloat/multilingual-e5-large\",\n",
    ")\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
